# -*- coding: utf-8 -*-
"""DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DrWdm57jbdcVXraeFKjIfU7qrpviY3zK
"""

from IPython.core.display import set_matplotlib_formats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy.stats import mode
np.random.seed(1234)

class Node:
    def __init__(self,indices,parent):
      self.indices = indices
      self.left = None
      self.right = None
      self.split_feature = None
      self.split_value = None
      if parent:
        self.depth = parent.depth+1
        self.num_classes = parent.num_classes
        self.data = parent.data
        self.labels = parent.labels
        class_probs = np.bincount(self.labels[indices],minlength = self.num_classes)
        self.class_probs = class_probs/np.sum(class_probs)

def greedy_test(node,cost_func):
    best_cost = np.inf
    best_feature, best_value = None, None
    num_instances,num_features = node.data.shape
    #sort the features to get the test value candidates 
    #taking avg of consecutive sorted feature values
    data_sorted = np.sort(node.data[node.indices],axis=0)
    test_candidates = (data_sorted[1:]+data_sorted[:-1])/2.
    for f in range(num_features):
      data_f = node.data[node.indices,f]
      for test in test_candidates[:,f]:
          # store the data corresponding to the f-th feature
          left_ind = node.indices[data_f <= test]
          right_ind = node.indices[data_f > test]
          # stop splitting when the child has zero element
          if len(left_ind)==0 or len(right_ind)==0:
              continue
          #compute the left and right cost based on the current split
          left_cost = cost_func(node.labels[left_ind])
          right_cost = cost_func(node.labels[right_ind])
          num_left, num_right = left_ind.shape[0], right_ind.shape[0]
          #the weighted sum of left and right cost
          cost = (num_left*left_cost+num_right*right_cost)/num_instances
          # updates the lowest cost
          if cost<best_cost:
              best_cost = cost
              best_feature = f
              best_value = test
    return best_cost,best_feature,best_value

def cost_misclassification(labels):
    #subtract the maximum prob of any class
    counts = np.bincount(labels)
    class_probs = counts/np.sum(counts)
    return 1-np.max(class_probs)

def cost_entropy(labels):
    #class probabilities
    class_probs = np.bincount(labels)/len(labels)
    class_probs = class_probs[class_probs>0]
    return -np.sum(class_probs*np.log(class_probs))

def cost_gini_index(labels):
    class_probs = np.bincount(labels)/len(labels)
    return 1-np.sum(np.square(class_probs))

class DecisionTree:
    def __init__(self,num_classes=None, max_depth=3,cost_func=cost_misclassification,min_leaf_instances=1):
        self.max_depth = max_depth
        self.root = None
        self.cost_func = cost_func
        self.num_classes = num_classes
        self.min_leaf_instances = min_leaf_instances
    def fit(self, data, labels):
        pass                            #pass in python 3 means nothing happens and the method here is empty
    
    def predict(self, data_test):
        pass
    
def fit(self,data,labels):
        self.data = data
        self.labels = labels
        if self.num_classes is None:
            self.num_classes = np.max(labels)+1
        # initialize the root
        self.root = Node(np.arange(data.shape[0]),None)
        self.root.data = data
        self.root.labels = labels
        self.root.num_classes = self.num_classes
        self.root.depth =0
        self._fit_tree(self.root)
        return self

def _fit_tree(self,node):
        # terminate consition : leaf node
        if node.depth == self.max_depth or len(node.indices) <= self.min_leaf_instances:
           return
        #greedy alg to select the best test by minimizing the cost
        cost,split_feature,split_value = greedy_test(node,self.cost_func)
        # inifinity means that not possible to split
        if np.isinf(cost):
          return
        test = node.data[node.indices,split_feature] <= split_value
        #store the split feature and value 
        node.split_feature = split_feature
        node.split_value = split_value

        left = Node(node.indices[test],node)
        right = Node(node.indices[np.logical_not(test)],node)
        #recursively call fit_tree function
        self._fit_tree(left)
        self._fit_tree(right)
        node.left = left
        node.right = right

DecisionTree.fit = fit
DecisionTree._fit_tree = _fit_tree

def predict(self,data_test):
        class_probs = np.zeros((data_test.shape[0],self.num_classes))
        for n,x in enumerate(data_test):
          node = self.root
          #loop along the depth of the tree where the parent data sample fall in 
          #based on the split
          while node.left:
            if x[node.split_feature] <= node.split_value:
                node = node.left
            else:
                node = node.right
          class_probs[n,:] = node.class_probs
        return class_probs
    
def evaluate_acc(self,y_test,y_pred):
    accuracy = np.sum(y_pred == y_test)/y_test.shape[0]
    print(f'accuracy is {accuracy*100:.1f}.')
    return accuracy

from sklearn import datasets

DecisionTree.predict = predict

dataset = datasets.load_iris()
x,y = dataset['data'][:,:2],dataset['target']
(num_instances,num_features), num_classes=x.shape, np.max(y)+1
inds = np.random.permutation(num_instances)
x_train,y_train = x[inds[:100]],y[inds[:100]]
x_test,y_test = x[inds[100:]],y[inds[100:]]

tree = DecisionTree(max_depth=20)
probs_test = tree.fit(x_train,y_train).predict(x_test)
y_pred = np.argmax(probs_test,1)
accuracy = np.sum(y_pred == y_test)/y_test.shape[0]
print(f'accuracy is {accuracy*100:.1f}.')
