{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DecisionTree.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "h-lKYp04vYbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb6dfb2-7a1d-4c77-84b1-54ae4ab6bf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115 data used to train\n",
            "accuracy is 58.9.\n",
            "predict with test set\n",
            "accuracy is 90.4.\n",
            "predict with training set\n",
            "230 data used to train\n",
            "accuracy is 60.0.\n",
            "predict with test set\n",
            "accuracy is 85.2.\n",
            "predict with training set\n",
            "345 data used to train\n",
            "accuracy is 63.3.\n",
            "predict with test set\n",
            "accuracy is 81.7.\n",
            "predict with training set\n",
            "460 data used to train\n",
            "accuracy is 64.4.\n",
            "predict with test set\n",
            "accuracy is 80.9.\n",
            "predict with training set\n",
            "576 data used to train\n",
            "accuracy is 68.2.\n",
            "predict with test set\n",
            "accuracy is 79.5.\n",
            "predict with training set\n",
            "691 data used to train\n",
            "accuracy is 67.0.\n",
            "predict with test set\n",
            "accuracy is 79.5.\n",
            "predict with training set\n",
            "806 data used to train\n",
            "accuracy is 67.5.\n",
            "predict with test set\n",
            "accuracy is 79.5.\n",
            "predict with training set\n",
            "921 data used to train\n",
            "accuracy is 66.5.\n",
            "predict with test set\n",
            "accuracy is 78.6.\n",
            "predict with training set\n",
            "1036 data used to train\n",
            "accuracy is 65.2.\n",
            "predict with test set\n",
            "accuracy is 79.1.\n",
            "predict with training set\n",
            "---------------------------------------------\n",
            "8 data used to train\n",
            "accuracy is 70.8.\n",
            "predict with test set\n",
            "accuracy is 75.0.\n",
            "predict with training set\n",
            "16 data used to train\n",
            "accuracy is 65.6.\n",
            "predict with test set\n",
            "accuracy is 81.2.\n",
            "predict with training set\n",
            "24 data used to train\n",
            "accuracy is 62.5.\n",
            "predict with test set\n",
            "accuracy is 87.5.\n",
            "predict with training set\n",
            "32 data used to train\n",
            "accuracy is 62.5.\n",
            "predict with test set\n",
            "accuracy is 84.4.\n",
            "predict with training set\n",
            "40 data used to train\n",
            "accuracy is 60.0.\n",
            "predict with test set\n",
            "accuracy is 82.5.\n",
            "predict with training set\n",
            "48 data used to train\n",
            "accuracy is 59.4.\n",
            "predict with test set\n",
            "accuracy is 77.1.\n",
            "predict with training set\n",
            "56 data used to train\n",
            "accuracy is 50.0.\n",
            "predict with test set\n",
            "accuracy is 80.4.\n",
            "predict with training set\n",
            "64 data used to train\n",
            "accuracy is 50.0.\n",
            "predict with test set\n",
            "accuracy is 78.1.\n",
            "predict with training set\n",
            "72 data used to train\n",
            "accuracy is 25.0.\n",
            "predict with test set\n",
            "accuracy is 79.2.\n",
            "predict with training set\n"
          ]
        }
      ],
      "source": [
        "from IPython.core.display import set_matplotlib_formats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from cProfile import label\n",
        "from scipy.io import arff\n",
        "from scipy.stats import mode\n",
        "np.random.seed(1234)\n",
        "\n",
        "def task1():\n",
        "    data3 = arff.loadarff('./sample_data/messidor_features.arff')\n",
        "    data3 = pd.DataFrame(data3[0])\n",
        "    col_names = ['qual_assess','pre_screen','MA_detection_.5','MA_detection_.6','MA_detection_.7','MA_detection_.8',\n",
        "            'MA_detection_.9','MA_detection_1.0','exudate_detection_.3','exudate_detection_.4','exudate_detection_.5','exudate_detection_.6'\n",
        "            ,'exudate_detection_.7','exudate_detection_.8','exudate_detection_.9','exudate_detection_1.0',\n",
        "            'euc_dist','diam_opt_disc','AM/FM','class_label']\n",
        "    data3.columns = col_names\n",
        "    data3.class_label = data3.class_label.apply(lambda x: pd.to_numeric(str(x)[2]))\n",
        "    data3 = data3.replace({'?' : np.nan}).dropna()\n",
        "    \n",
        "    # print(df.corr())\n",
        "    # print(df)\n",
        "    # basic statistics df\n",
        "    # print(df.describe(include='all'))\n",
        "    # Document messidor features.arff above\n",
        "    # Document hepatitis below\n",
        "    df2 = pd.read_csv('./sample_data/hepatitis1.csv', \n",
        "    names=[\"class\", \"age\", \"sex\", \"steroid\", \"antivirals\", \n",
        "    \"fatigue\", \"malaise\", \"anorexia\", \"liver_big\", \"liver_firm\", \n",
        "    \"spleen_palpable\", \"spiders\", \"ascites\", \"varices\", \"bilirubin\", \n",
        "    \"alk_phosphate\", \"sgot\", \"albumin\", \"protime\", \"histology\"])\n",
        "    df2 = df2.replace({'?': np.nan}).dropna()\n",
        "    af = df2.select_dtypes(include=\"object\")\n",
        "    dic = {}\n",
        "    for i in af:\n",
        "      df2[i] = df2[i].astype(float, errors = \"raise\")\n",
        "    df2.astype(dic).dtypes\n",
        "    \n",
        "    # print(df2.corr())\n",
        "    # print(df2)\n",
        "    # basic statistics df2\n",
        "    # print(df2.describe(include='all'))\n",
        "\n",
        "    # data3 = data3.to_numpy()\n",
        "    # df2 = df2.to_numpy()\n",
        "    # print(df[:, 18:]) # taking values that are besides 1 and 0\n",
        "    # print(df2[:, 16:]) # taking values that are besides 1 and 0\n",
        "    return data3, df2 # messidor and hep data respectively\n",
        "\n",
        "class Node:\n",
        "    def __init__(self,indices,parent):\n",
        "      self.indices = indices\n",
        "      self.left = None\n",
        "      self.right = None\n",
        "      self.split_feature = None\n",
        "      self.split_value = None\n",
        "      if parent:\n",
        "        self.depth = parent.depth+1\n",
        "        self.num_classes = parent.num_classes\n",
        "        self.data = parent.data\n",
        "        self.labels = parent.labels\n",
        "        class_probs = np.bincount(self.labels[indices],minlength = self.num_classes)\n",
        "        self.class_probs = class_probs/np.sum(class_probs)\n",
        "\n",
        "def greedy_test(node,cost_func):\n",
        "    best_cost = np.inf\n",
        "    best_feature, best_value = None, None\n",
        "    num_instances,num_features = node.data.shape\n",
        "    #sort the features to get the test value candidates \n",
        "    #taking avg of consecutive sorted feature values\n",
        "    data_sorted = np.sort(node.data[node.indices],axis=0)\n",
        "    test_candidates = (data_sorted[1:]+data_sorted[:-1])/2.\n",
        "    for f in range(num_features):\n",
        "      data_f = node.data[node.indices,f]\n",
        "      for test in test_candidates[:,f]:\n",
        "          # store the data corresponding to the f-th feature\n",
        "          left_ind = node.indices[data_f <= test]\n",
        "          right_ind = node.indices[data_f > test]\n",
        "          # stop splitting when the child has zero element\n",
        "          if len(left_ind)==0 or len(right_ind)==0:\n",
        "              continue\n",
        "          #compute the left and right cost based on the current split\n",
        "          left_cost = cost_func(node.labels[left_ind])\n",
        "          right_cost = cost_func(node.labels[right_ind])\n",
        "          num_left, num_right = left_ind.shape[0], right_ind.shape[0]\n",
        "          #the weighted sum of left and right cost\n",
        "          cost = (num_left*left_cost+num_right*right_cost)/num_instances\n",
        "          # updates the lowest cost\n",
        "          if cost<best_cost:\n",
        "              best_cost = cost\n",
        "              best_feature = f\n",
        "              best_value = test\n",
        "    return best_cost,best_feature,best_value\n",
        "\n",
        "def cost_misclassification(labels):\n",
        "    #subtract the maximum prob of any class\n",
        "    counts = np.bincount(labels)\n",
        "    class_probs = counts/np.sum(counts)\n",
        "    return 1-np.max(class_probs)\n",
        "\n",
        "def cost_entropy(labels):\n",
        "    #class probabilities\n",
        "    class_probs = np.bincount(labels)/len(labels)\n",
        "    class_probs = class_probs[class_probs>0]\n",
        "    return -np.sum(class_probs*np.log(class_probs))\n",
        "\n",
        "def cost_gini_index(labels):\n",
        "    class_probs = np.bincount(labels)/len(labels)\n",
        "    return 1-np.sum(np.square(class_probs))\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self,num_classes=None, max_depth=3,cost_func=cost_misclassification,min_leaf_instances=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "        self.cost_func = cost_func\n",
        "        self.num_classes = num_classes\n",
        "        self.min_leaf_instances = min_leaf_instances\n",
        "    def fit(self, data, labels):\n",
        "        pass                            #pass in python 3 means nothing happens and the method here is empty\n",
        "    \n",
        "    def predict(self, data_test):\n",
        "        pass\n",
        "    \n",
        "def fit(self,data,labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        if self.num_classes is None:\n",
        "            self.num_classes = np.max(labels)+1\n",
        "        # initialize the root\n",
        "        self.root = Node(np.arange(data.shape[0]),None)\n",
        "        self.root.data = data\n",
        "        self.root.labels = labels\n",
        "        self.root.num_classes = self.num_classes\n",
        "        self.root.depth =0\n",
        "        self._fit_tree(self.root)\n",
        "        return self\n",
        "\n",
        "def _fit_tree(self,node):\n",
        "        # terminate consition : leaf node\n",
        "        if node.depth == self.max_depth or len(node.indices) <= self.min_leaf_instances:\n",
        "           return\n",
        "        #greedy alg to select the best test by minimizing the cost\n",
        "        cost,split_feature,split_value = greedy_test(node,self.cost_func)\n",
        "        # inifinity means that not possible to split\n",
        "        if np.isinf(cost):\n",
        "          return\n",
        "        test = node.data[node.indices,split_feature] <= split_value\n",
        "        #store the split feature and value \n",
        "        node.split_feature = split_feature\n",
        "        node.split_value = split_value\n",
        "\n",
        "        left = Node(node.indices[test],node)\n",
        "        right = Node(node.indices[np.logical_not(test)],node)\n",
        "        #recursively call fit_tree function\n",
        "        self._fit_tree(left)\n",
        "        self._fit_tree(right)\n",
        "        node.left = left\n",
        "        node.right = right\n",
        "\n",
        "DecisionTree.fit = fit\n",
        "DecisionTree._fit_tree = _fit_tree\n",
        "\n",
        "def predict(self,data_test):\n",
        "        class_probs = np.zeros((data_test.shape[0],self.num_classes))\n",
        "        for n,x in enumerate(data_test):\n",
        "          node = self.root\n",
        "          #loop along the depth of the tree where the parent data sample fall in \n",
        "          #based on the split\n",
        "          while node.left:\n",
        "            if x[node.split_feature] <= node.split_value:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "          class_probs[n,:] = node.class_probs\n",
        "        return class_probs\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# Main code below\n",
        "# -----------------\n",
        "\n",
        "\n",
        "def evaluate_acc(self,y_test,y_pred,x_test,x_train):\n",
        "    accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "    print(f'accuracy is {accuracy*100:.1f}.')\n",
        "    # return accuracy\n",
        "    # correct = y_test == y_pred\n",
        "    # incorrect = np.logical_not(correct)\n",
        "    # plt.scatter(x_train[:,0], x_train[:,1], c=y_train, marker='o', alpha=.2, label='train')\n",
        "    # plt.scatter(x_test[correct,0], x_test[correct,1], marker='.', c=y_pred[correct], label='correct')\n",
        "    # plt.scatter(x_test[incorrect,0], x_test[incorrect,1], marker='x', c=y_test[incorrect], label='misclassified')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "DecisionTree.predict = predict\n",
        "\n",
        "\n",
        "mess_data, hep_data = task1()  # get two datasets\n",
        "    \n",
        "# mess_data = mess_data.astype(np.float64)\n",
        "# hep_data = hep_data.astype(np.float64)\n",
        "\n",
        "\n",
        "# ----------------- data process ----------------------\n",
        "\n",
        "\n",
        "# Train with different train set and get accuracy(compare with test and training set) <- messidor_features datasets\n",
        "\n",
        "x = mess_data.drop(\"class_label\",axis = 1).values\n",
        "\n",
        "# looks like below\n",
        "# [[ 1.        1.       22.       ...  0.486903  0.100025  1.      ]\n",
        "#  [ 1.        1.       24.       ...  0.520908  0.144414  0.      ]\n",
        "#  [ 1.        1.       62.       ...  0.530904  0.128548  0.      ]\n",
        "#  ...\n",
        "#  [ 1.        0.       49.       ...  0.560632  0.129843  0.      ]\n",
        "#  [ 1.        1.       39.       ...  0.485972  0.10669   1.      ]\n",
        "#  [ 1.        1.        7.       ...  0.556192  0.088957  0.      ]]\n",
        "x = x[:, [2,3]]\n",
        "# [[22. 22.]\n",
        "#  [24. 24.]\n",
        "#  [62. 60.]\n",
        "#  ...\n",
        "#  [49. 48.]\n",
        "#  [39. 36.]\n",
        "#  [ 7.  7.]]\n",
        "y = mess_data['class_label'].values\n",
        "# [0 0 1 ... 0 1 0]\n",
        "\n",
        "for i in range(1,10):\n",
        "  train_portion = i/10\n",
        "\n",
        "\n",
        "  num_classes = 2\n",
        "\n",
        "  num_of_instances = x.shape[0]\n",
        "  num_of_features = x.shape[1]\n",
        "\n",
        "  num_train_portion = round(train_portion * num_of_instances)\n",
        "  print(num_train_portion,\"data used to train\")\n",
        "  shuffle = np.random.RandomState(seed = 1234).permutation(num_of_instances) \n",
        "  # Mix all the instances by random to better train the datasets\n",
        "  # Now split the datasets to train\n",
        "  x_train, y_train, x_test, y_test = x[shuffle[:num_train_portion]], y[shuffle[:num_train_portion]], x[shuffle[num_train_portion:]], y[shuffle[num_train_portion:]]\n",
        "  # Now the datasets has been splited\n",
        "  # below initialize tree object\n",
        "  tree = DecisionTree(max_depth = 20)\n",
        "  # Accuracy with test set\n",
        "  test = tree.fit(x_train, y_train).predict(x_test)\n",
        "  y_pred = np.argmax(test,1)\n",
        "  evaluate_acc(tree, y_test,y_pred,x_test,x_train)\n",
        "  print(\"predict with test set\")\n",
        "  # Accuracy with train set\n",
        "  test = tree.fit(x_train, y_train).predict(x_train)\n",
        "  y_pred = np.argmax(test,1)\n",
        "  evaluate_acc(tree, y_train,y_pred,x_test,x_train)\n",
        "  print(\"predict with training set\")\n",
        "\n",
        "# Train with different train set and get accuracy(compare with test and training set) <- hepatitis1 datasets\n",
        "print(\"---------------------------------------------\")\n",
        "x_2 = hep_data.drop(\"class\",axis = 1).values\n",
        "x_2 = x_2[:, [11,16]]\n",
        "y_2 = hep_data[\"histology\"].values\n",
        "\n",
        "for i in range(1,10):\n",
        "  train_portion = i/10\n",
        "\n",
        "\n",
        "  num_classes = 2\n",
        "\n",
        "  num_of_instances = x_2.shape[0]\n",
        "  num_of_features = x_2.shape[1]\n",
        "\n",
        "  num_train_portion = round(train_portion * num_of_instances)\n",
        "  print(num_train_portion,\"data used to train\")\n",
        "  shuffle = np.random.RandomState(seed = 1234).permutation(num_of_instances) \n",
        "  # Mix all the instances by random to better train the datasets\n",
        "  # Now split the datasets to train\n",
        "  x_train_2, y_train_2, x_test_2, y_test_2 = x_2[shuffle[:num_train_portion]], y_2[shuffle[:num_train_portion]], x_2[shuffle[num_train_portion:]], y_2[shuffle[num_train_portion:]]\n",
        "  # Now the datasets has been splited\n",
        "  # below initialize tree object\n",
        "  tree = DecisionTree(max_depth = 20)\n",
        "  # Accuracy with test set\n",
        "  test_2 = tree.fit(x_train_2, y_train_2).predict(x_test_2)\n",
        "  y_pred_2 = np.argmax(test_2,1)\n",
        "  evaluate_acc(tree, y_test_2,y_pred_2,x_test_2,x_train_2)\n",
        "  print(\"predict with test set\")\n",
        "  # Accuracy with train set\n",
        "  test_2 = tree.fit(x_train_2, y_train_2).predict(x_train_2)\n",
        "  y_pred_2 = np.argmax(test_2,1)\n",
        "  evaluate_acc(tree, y_train_2,y_pred_2,x_test_2,x_train_2)\n",
        "  print(\"predict with training set\")\n",
        "\n",
        "# hepatitis dataset\n",
        "# for i in range(1, max_depth):\n",
        "#   print(\"depth = \", i)\n",
        "#   tree = DecisionTree(max_depth=i)\n",
        "#   probs_test = tree.fit(hep_x_train,hep_y_train).predict(hep_x_test)\n",
        "#   y_pred = np.argmax(probs_test,1)\n",
        "#   accuracy = np.sum(y_pred == hep_y_test)/hep_y_test.shape[0]\n",
        "#   print(f'accuracy is {accuracy*100:.1f}.')\n",
        "#   # print(\"Messidore Accuracy: \", task2(i, mess_x_train, mess_y_train, mess_x_test))\n",
        "#   #print(\"Hepatitis Accuracy: \", decisionTreeTask(i, hep_x_train, hep_y_train, hep_x_test))\n",
        "#   print(\"\\n\")\n",
        "# dataset = datasets.load_iris()\n",
        "# x,y = dataset['data'][:,:2],dataset['target']\n",
        "# (num_instances,num_features), num_classes=x.shape, np.max(y)+1\n",
        "# inds = np.random.permutation(num_instances)\n",
        "# x_train,y_train = x[inds[:100]],y[inds[:100]]\n",
        "# x_test,y_test = x[inds[100:]],y[inds[100:]]\n",
        "\n",
        "# tree = DecisionTree(max_depth=20)\n",
        "# probs_test = tree.fit(x_train,y_train).predict(x_test)\n",
        "# y_pred = np.argmax(probs_test,1)\n",
        "# accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "# print(f'accuracy is {accuracy*100:.1f}.')"
      ]
    }
  ]
}